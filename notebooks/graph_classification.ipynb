{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification with GNN\n",
    "- Based off of the tutorial [Graph Classification with Graph Neural Networks](https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# For CUDA GPUs (ignore if CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download TU Dortmund University's MUTAG Dataset\n",
    "- [More info on MUTAG](https://paperswithcode.com/dataset/mutag)\n",
    "\n",
    "In particular, MUTAG is a collection of nitroaromatic compounds and the goal is to predict their mutagenicity on Salmonella typhimurium. Input graphs are used to represent chemical compounds, where vertices stand for atoms and are labeled by the atom type (represented by one-hot encoding), while edges between vertices represent bonds between the corresponding atoms. It includes 188 samples of chemical compounds with 7 discrete node labels.\n",
    "\n",
    "The MUTAG dataset consists of 188 chemical compounds divided into two \n",
    "classes according to their mutagenic effect on a bacterium. \n",
    "\n",
    "The chemical data was obtained form http://cdb.ics.uci.edu and converted \n",
    "to graphs, where vertices represent atoms and edges represent chemical \n",
    "bonds. Explicit hydrogen atoms have been removed and vertices are labeled\n",
    "by atom type and edges by bond type (single, double, triple or aromatic).\n",
    "Chemical data was processed using the Chemistry Development Kit (v1.4).\n",
    "\n",
    "Node labels:\n",
    "\n",
    "- 0  C\n",
    "- 1  N\n",
    "- 2  O\n",
    "- 3  F\n",
    "- 4  I\n",
    "- 5  Cl\n",
    "- 6  Br\n",
    "\n",
    "Edge labels:\n",
    "\n",
    "- 0  aromatic\n",
    "- 1  single\n",
    "- 2  double\n",
    "- 3  triple\n",
    "\n",
    "This dataset provides 188 different graphs, and the task is to classify each graph into one out of two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: MUTAG(188):\n",
      "====================\n",
      "Number of graphs: 188\n",
      "Number of features: 7\n",
      "Number of classes: 2\n",
      "\n",
      "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 17\n",
      "Number of edges: 38\n",
      "Average node degree: 2.24\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='../data/TUDataset', name='MUTAG')\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Training/Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 150\n",
      "Number of test graphs: 38\n"
     ]
    }
   ],
   "source": [
    "split_index = 150\n",
    "\n",
    "torch.manual_seed(42)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:split_index]\n",
    "test_dataset  = dataset[split_index:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN Minibatching with Dataloader\n",
    "- Regular mini-batching is computationally expensive\n",
    "    - Adding another dimension to an adjacency matrix will increase memory consumption\n",
    "\n",
    "- Instead, we stack adjacency matrices diagonally along a larger matrix\n",
    "    - Message passing is not affected since the subgraphs are disconnected\n",
    "    - No overhead because adjacency matrices are stored sparsely (0s are ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 2604], x=[1172, 7], edge_attr=[2604, 4], y=[64], batch=[1172], ptr=[65])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 64\n",
      "DataBatch(edge_index=[2, 2536], x=[1155, 7], edge_attr=[2536, 4], y=[64], batch=[1155], ptr=[65])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 22\n",
      "DataBatch(edge_index=[2, 836], x=[376, 7], edge_attr=[836, 4], y=[22], batch=[376], ptr=[23])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Training\n",
    "Training a GNN for graph classification usually follows a simple recipe:\n",
    "\n",
    "1. Embed each node by performing multiple rounds of message passing\n",
    "2. Aggregate node embeddings into a unified graph embedding (**readout layer**)\n",
    "3. Train a final classifier on the graph embedding\n",
    "\n",
    "There exists multiple **readout layers** in literature, but the most common one is to simply take the average of node embeddings:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{\\mathcal{G}} = \\frac{1}{|\\mathcal{V}|} \\sum_{v \\in \\mathcal{V}} \\mathcal{x}^{(L)}_v\n",
    "$$\n",
    "\n",
    "PyTorch Geometric provides this functionality via [`torch_geometric.nn.global_mean_pool`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.global_mean_pool), which takes in the node embeddings of all nodes in the mini-batch and the assignment vector `batch` to compute a graph embedding of size `[batch_size, hidden_channels]` for each graph in the batch.\n",
    "\n",
    "The final architecture for applying GNNs to the task of graph classification then looks as follows and allows for complete end-to-end training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(7, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.6933, Test Acc: 0.5526\n",
      "Epoch: 002, Train Acc: 0.6933, Test Acc: 0.5526\n",
      "Epoch: 003, Train Acc: 0.6933, Test Acc: 0.5526\n",
      "Epoch: 004, Train Acc: 0.6933, Test Acc: 0.5526\n",
      "Epoch: 005, Train Acc: 0.6933, Test Acc: 0.5526\n",
      "Epoch: 006, Train Acc: 0.7000, Test Acc: 0.5789\n",
      "Epoch: 007, Train Acc: 0.7600, Test Acc: 0.6316\n",
      "Epoch: 008, Train Acc: 0.7600, Test Acc: 0.6842\n",
      "Epoch: 009, Train Acc: 0.7267, Test Acc: 0.6842\n",
      "Epoch: 010, Train Acc: 0.7533, Test Acc: 0.6579\n",
      "Epoch: 011, Train Acc: 0.7333, Test Acc: 0.6579\n",
      "Epoch: 012, Train Acc: 0.7467, Test Acc: 0.6842\n",
      "Epoch: 013, Train Acc: 0.7667, Test Acc: 0.7105\n",
      "Epoch: 014, Train Acc: 0.7800, Test Acc: 0.6842\n",
      "Epoch: 015, Train Acc: 0.7600, Test Acc: 0.7105\n",
      "Epoch: 016, Train Acc: 0.7667, Test Acc: 0.6842\n",
      "Epoch: 017, Train Acc: 0.7667, Test Acc: 0.6842\n",
      "Epoch: 018, Train Acc: 0.7733, Test Acc: 0.6316\n",
      "Epoch: 019, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 020, Train Acc: 0.7800, Test Acc: 0.6842\n",
      "Epoch: 021, Train Acc: 0.7733, Test Acc: 0.6579\n",
      "Epoch: 022, Train Acc: 0.7600, Test Acc: 0.6842\n",
      "Epoch: 023, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 024, Train Acc: 0.7867, Test Acc: 0.7105\n",
      "Epoch: 025, Train Acc: 0.7800, Test Acc: 0.6579\n",
      "Epoch: 026, Train Acc: 0.7800, Test Acc: 0.6579\n",
      "Epoch: 027, Train Acc: 0.7800, Test Acc: 0.6579\n",
      "Epoch: 028, Train Acc: 0.7667, Test Acc: 0.6579\n",
      "Epoch: 029, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 030, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 031, Train Acc: 0.7667, Test Acc: 0.6579\n",
      "Epoch: 032, Train Acc: 0.7733, Test Acc: 0.6579\n",
      "Epoch: 033, Train Acc: 0.7667, Test Acc: 0.6579\n",
      "Epoch: 034, Train Acc: 0.7733, Test Acc: 0.7105\n",
      "Epoch: 035, Train Acc: 0.7867, Test Acc: 0.7368\n",
      "Epoch: 036, Train Acc: 0.7733, Test Acc: 0.6579\n",
      "Epoch: 037, Train Acc: 0.7733, Test Acc: 0.6579\n",
      "Epoch: 038, Train Acc: 0.7667, Test Acc: 0.6579\n",
      "Epoch: 039, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 040, Train Acc: 0.7600, Test Acc: 0.6579\n",
      "Epoch: 041, Train Acc: 0.7467, Test Acc: 0.6579\n",
      "Epoch: 042, Train Acc: 0.7667, Test Acc: 0.6579\n",
      "Epoch: 043, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 044, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 045, Train Acc: 0.7600, Test Acc: 0.6579\n",
      "Epoch: 046, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 047, Train Acc: 0.7867, Test Acc: 0.7105\n",
      "Epoch: 048, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 049, Train Acc: 0.7667, Test Acc: 0.6579\n",
      "Epoch: 050, Train Acc: 0.7600, Test Acc: 0.6842\n",
      "Epoch: 051, Train Acc: 0.7733, Test Acc: 0.7105\n",
      "Epoch: 052, Train Acc: 0.7867, Test Acc: 0.6316\n",
      "Epoch: 053, Train Acc: 0.7533, Test Acc: 0.6579\n",
      "Epoch: 054, Train Acc: 0.7867, Test Acc: 0.7105\n",
      "Epoch: 055, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 056, Train Acc: 0.7533, Test Acc: 0.6579\n",
      "Epoch: 057, Train Acc: 0.7800, Test Acc: 0.6579\n",
      "Epoch: 058, Train Acc: 0.7533, Test Acc: 0.6579\n",
      "Epoch: 059, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 060, Train Acc: 0.7800, Test Acc: 0.6579\n",
      "Epoch: 061, Train Acc: 0.7800, Test Acc: 0.6579\n",
      "Epoch: 062, Train Acc: 0.7600, Test Acc: 0.6579\n",
      "Epoch: 063, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 064, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 065, Train Acc: 0.7800, Test Acc: 0.6842\n",
      "Epoch: 066, Train Acc: 0.7667, Test Acc: 0.6579\n",
      "Epoch: 067, Train Acc: 0.7667, Test Acc: 0.6842\n",
      "Epoch: 068, Train Acc: 0.7667, Test Acc: 0.6842\n",
      "Epoch: 069, Train Acc: 0.8000, Test Acc: 0.6053\n",
      "Epoch: 070, Train Acc: 0.7867, Test Acc: 0.6579\n",
      "Epoch: 071, Train Acc: 0.7733, Test Acc: 0.6579\n",
      "Epoch: 072, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 073, Train Acc: 0.7800, Test Acc: 0.6842\n",
      "Epoch: 074, Train Acc: 0.7733, Test Acc: 0.6579\n",
      "Epoch: 075, Train Acc: 0.7800, Test Acc: 0.6579\n",
      "Epoch: 076, Train Acc: 0.7800, Test Acc: 0.6579\n",
      "Epoch: 077, Train Acc: 0.7867, Test Acc: 0.6579\n",
      "Epoch: 078, Train Acc: 0.7800, Test Acc: 0.6579\n",
      "Epoch: 079, Train Acc: 0.7733, Test Acc: 0.6579\n",
      "Epoch: 080, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 081, Train Acc: 0.7800, Test Acc: 0.6579\n",
      "Epoch: 082, Train Acc: 0.7800, Test Acc: 0.6842\n",
      "Epoch: 083, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 084, Train Acc: 0.8000, Test Acc: 0.6316\n",
      "Epoch: 085, Train Acc: 0.7800, Test Acc: 0.6579\n",
      "Epoch: 086, Train Acc: 0.7733, Test Acc: 0.7105\n",
      "Epoch: 087, Train Acc: 0.7667, Test Acc: 0.6842\n",
      "Epoch: 088, Train Acc: 0.7667, Test Acc: 0.6842\n",
      "Epoch: 089, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 090, Train Acc: 0.7867, Test Acc: 0.7105\n",
      "Epoch: 091, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 092, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 093, Train Acc: 0.7867, Test Acc: 0.6579\n",
      "Epoch: 094, Train Acc: 0.7867, Test Acc: 0.6053\n",
      "Epoch: 095, Train Acc: 0.7867, Test Acc: 0.6579\n",
      "Epoch: 096, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 097, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 098, Train Acc: 0.7867, Test Acc: 0.6579\n",
      "Epoch: 099, Train Acc: 0.7800, Test Acc: 0.6842\n",
      "Epoch: 100, Train Acc: 0.7600, Test Acc: 0.7105\n",
      "Epoch: 101, Train Acc: 0.7933, Test Acc: 0.6579\n",
      "Epoch: 102, Train Acc: 0.7933, Test Acc: 0.6316\n",
      "Epoch: 103, Train Acc: 0.7667, Test Acc: 0.6842\n",
      "Epoch: 104, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 105, Train Acc: 0.7933, Test Acc: 0.6842\n",
      "Epoch: 106, Train Acc: 0.7933, Test Acc: 0.6842\n",
      "Epoch: 107, Train Acc: 0.7933, Test Acc: 0.6842\n",
      "Epoch: 108, Train Acc: 0.7933, Test Acc: 0.6579\n",
      "Epoch: 109, Train Acc: 0.7867, Test Acc: 0.6842\n",
      "Epoch: 110, Train Acc: 0.7867, Test Acc: 0.6842\n",
      "Epoch: 111, Train Acc: 0.7933, Test Acc: 0.6579\n",
      "Epoch: 112, Train Acc: 0.7867, Test Acc: 0.6316\n",
      "Epoch: 113, Train Acc: 0.7867, Test Acc: 0.6316\n",
      "Epoch: 114, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 115, Train Acc: 0.7800, Test Acc: 0.6842\n",
      "Epoch: 116, Train Acc: 0.7867, Test Acc: 0.6842\n",
      "Epoch: 117, Train Acc: 0.7933, Test Acc: 0.6842\n",
      "Epoch: 118, Train Acc: 0.7933, Test Acc: 0.6579\n",
      "Epoch: 119, Train Acc: 0.7867, Test Acc: 0.6842\n",
      "Epoch: 120, Train Acc: 0.7933, Test Acc: 0.6842\n",
      "Epoch: 121, Train Acc: 0.7800, Test Acc: 0.6842\n",
      "Epoch: 122, Train Acc: 0.7800, Test Acc: 0.6842\n",
      "Epoch: 123, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 124, Train Acc: 0.7867, Test Acc: 0.6842\n",
      "Epoch: 125, Train Acc: 0.7800, Test Acc: 0.6842\n",
      "Epoch: 126, Train Acc: 0.7933, Test Acc: 0.6842\n",
      "Epoch: 127, Train Acc: 0.8000, Test Acc: 0.6842\n",
      "Epoch: 128, Train Acc: 0.7867, Test Acc: 0.6842\n",
      "Epoch: 129, Train Acc: 0.7933, Test Acc: 0.6842\n",
      "Epoch: 130, Train Acc: 0.7867, Test Acc: 0.7105\n",
      "Epoch: 131, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 132, Train Acc: 0.7933, Test Acc: 0.6579\n",
      "Epoch: 133, Train Acc: 0.8000, Test Acc: 0.6316\n",
      "Epoch: 134, Train Acc: 0.7933, Test Acc: 0.6842\n",
      "Epoch: 135, Train Acc: 0.8000, Test Acc: 0.6579\n",
      "Epoch: 136, Train Acc: 0.8000, Test Acc: 0.6579\n",
      "Epoch: 137, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 138, Train Acc: 0.7867, Test Acc: 0.7105\n",
      "Epoch: 139, Train Acc: 0.8067, Test Acc: 0.6316\n",
      "Epoch: 140, Train Acc: 0.8000, Test Acc: 0.6579\n",
      "Epoch: 141, Train Acc: 0.7933, Test Acc: 0.6842\n",
      "Epoch: 142, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 143, Train Acc: 0.8000, Test Acc: 0.6842\n",
      "Epoch: 144, Train Acc: 0.7933, Test Acc: 0.6842\n",
      "Epoch: 145, Train Acc: 0.8133, Test Acc: 0.6842\n",
      "Epoch: 146, Train Acc: 0.8067, Test Acc: 0.6842\n",
      "Epoch: 147, Train Acc: 0.8000, Test Acc: 0.6842\n",
      "Epoch: 148, Train Acc: 0.8133, Test Acc: 0.6842\n",
      "Epoch: 149, Train Acc: 0.8000, Test Acc: 0.6842\n",
      "Epoch: 150, Train Acc: 0.8000, Test Acc: 0.6842\n",
      "Epoch: 151, Train Acc: 0.8067, Test Acc: 0.6579\n",
      "Epoch: 152, Train Acc: 0.8067, Test Acc: 0.6579\n",
      "Epoch: 153, Train Acc: 0.8267, Test Acc: 0.6579\n",
      "Epoch: 154, Train Acc: 0.8200, Test Acc: 0.6579\n",
      "Epoch: 155, Train Acc: 0.8067, Test Acc: 0.6579\n",
      "Epoch: 156, Train Acc: 0.8067, Test Acc: 0.6579\n",
      "Epoch: 157, Train Acc: 0.8000, Test Acc: 0.6579\n",
      "Epoch: 158, Train Acc: 0.8267, Test Acc: 0.6842\n",
      "Epoch: 159, Train Acc: 0.8333, Test Acc: 0.6842\n",
      "Epoch: 160, Train Acc: 0.8200, Test Acc: 0.6842\n",
      "Epoch: 161, Train Acc: 0.8000, Test Acc: 0.6579\n",
      "Epoch: 162, Train Acc: 0.8333, Test Acc: 0.7105\n",
      "Epoch: 163, Train Acc: 0.8200, Test Acc: 0.7105\n",
      "Epoch: 164, Train Acc: 0.8067, Test Acc: 0.6316\n",
      "Epoch: 165, Train Acc: 0.8133, Test Acc: 0.6579\n",
      "Epoch: 166, Train Acc: 0.8400, Test Acc: 0.7105\n",
      "Epoch: 167, Train Acc: 0.8200, Test Acc: 0.6842\n",
      "Epoch: 168, Train Acc: 0.8133, Test Acc: 0.6579\n",
      "Epoch: 169, Train Acc: 0.8333, Test Acc: 0.6842\n",
      "Epoch: 170, Train Acc: 0.8333, Test Acc: 0.6579\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving with GraphConv\n",
    "- No neighborhood normalization (decreases expressivity)\n",
    "- Adds skip-connection\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell+1)} = \\mathbf{W}^{(\\ell + 1)}_1 \\mathbf{x}_v^{(\\ell)} + \\mathbf{W}^{(\\ell + 1)}_2 \\sum_{w \\in \\mathcal{N}(v)} \\mathbf{x}_w^{(\\ell)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): GraphConv(7, 64)\n",
      "  (conv2): GraphConv(64, 64)\n",
      "  (conv3): GraphConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GraphConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GNN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): GraphConv(7, 64)\n",
      "  (conv2): GraphConv(64, 64)\n",
      "  (conv3): GraphConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Epoch: 001, Train Acc: 0.3067, Test Acc: 0.4474\n",
      "Epoch: 002, Train Acc: 0.6933, Test Acc: 0.5526\n",
      "Epoch: 003, Train Acc: 0.6933, Test Acc: 0.5526\n",
      "Epoch: 004, Train Acc: 0.6933, Test Acc: 0.5526\n",
      "Epoch: 005, Train Acc: 0.6933, Test Acc: 0.5526\n",
      "Epoch: 006, Train Acc: 0.7000, Test Acc: 0.5789\n",
      "Epoch: 007, Train Acc: 0.7733, Test Acc: 0.6842\n",
      "Epoch: 008, Train Acc: 0.7733, Test Acc: 0.6579\n",
      "Epoch: 009, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 010, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 011, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 012, Train Acc: 0.7667, Test Acc: 0.6842\n",
      "Epoch: 013, Train Acc: 0.8067, Test Acc: 0.7895\n",
      "Epoch: 014, Train Acc: 0.8000, Test Acc: 0.7895\n",
      "Epoch: 015, Train Acc: 0.7533, Test Acc: 0.6842\n",
      "Epoch: 016, Train Acc: 0.8133, Test Acc: 0.8158\n",
      "Epoch: 017, Train Acc: 0.7933, Test Acc: 0.7895\n",
      "Epoch: 018, Train Acc: 0.8000, Test Acc: 0.7895\n",
      "Epoch: 019, Train Acc: 0.8467, Test Acc: 0.7632\n",
      "Epoch: 020, Train Acc: 0.8000, Test Acc: 0.7632\n",
      "Epoch: 021, Train Acc: 0.8467, Test Acc: 0.8158\n",
      "Epoch: 022, Train Acc: 0.8000, Test Acc: 0.7368\n",
      "Epoch: 023, Train Acc: 0.8533, Test Acc: 0.8158\n",
      "Epoch: 024, Train Acc: 0.8067, Test Acc: 0.7632\n",
      "Epoch: 025, Train Acc: 0.8400, Test Acc: 0.7895\n",
      "Epoch: 026, Train Acc: 0.8533, Test Acc: 0.8158\n",
      "Epoch: 027, Train Acc: 0.8533, Test Acc: 0.7632\n",
      "Epoch: 028, Train Acc: 0.8467, Test Acc: 0.8158\n",
      "Epoch: 029, Train Acc: 0.8400, Test Acc: 0.8158\n",
      "Epoch: 030, Train Acc: 0.8000, Test Acc: 0.7105\n",
      "Epoch: 031, Train Acc: 0.9000, Test Acc: 0.8421\n",
      "Epoch: 032, Train Acc: 0.8533, Test Acc: 0.7895\n",
      "Epoch: 033, Train Acc: 0.8533, Test Acc: 0.7895\n",
      "Epoch: 034, Train Acc: 0.8533, Test Acc: 0.7632\n",
      "Epoch: 035, Train Acc: 0.8800, Test Acc: 0.8947\n",
      "Epoch: 036, Train Acc: 0.8533, Test Acc: 0.8158\n",
      "Epoch: 037, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 038, Train Acc: 0.8333, Test Acc: 0.6579\n",
      "Epoch: 039, Train Acc: 0.8667, Test Acc: 0.8421\n",
      "Epoch: 040, Train Acc: 0.8533, Test Acc: 0.7632\n",
      "Epoch: 041, Train Acc: 0.8400, Test Acc: 0.8421\n",
      "Epoch: 042, Train Acc: 0.8733, Test Acc: 0.7632\n",
      "Epoch: 043, Train Acc: 0.8800, Test Acc: 0.7632\n",
      "Epoch: 044, Train Acc: 0.8733, Test Acc: 0.8684\n",
      "Epoch: 045, Train Acc: 0.8600, Test Acc: 0.7632\n",
      "Epoch: 046, Train Acc: 0.9067, Test Acc: 0.8684\n",
      "Epoch: 047, Train Acc: 0.9200, Test Acc: 0.8158\n",
      "Epoch: 048, Train Acc: 0.9133, Test Acc: 0.7895\n",
      "Epoch: 049, Train Acc: 0.9133, Test Acc: 0.8158\n",
      "Epoch: 050, Train Acc: 0.9000, Test Acc: 0.8684\n",
      "Epoch: 051, Train Acc: 0.8667, Test Acc: 0.8158\n",
      "Epoch: 052, Train Acc: 0.9067, Test Acc: 0.7895\n",
      "Epoch: 053, Train Acc: 0.9200, Test Acc: 0.8684\n",
      "Epoch: 054, Train Acc: 0.9067, Test Acc: 0.8158\n",
      "Epoch: 055, Train Acc: 0.9133, Test Acc: 0.9211\n",
      "Epoch: 056, Train Acc: 0.9200, Test Acc: 0.8684\n",
      "Epoch: 057, Train Acc: 0.8800, Test Acc: 0.8684\n",
      "Epoch: 058, Train Acc: 0.9200, Test Acc: 0.7895\n",
      "Epoch: 059, Train Acc: 0.9267, Test Acc: 0.9211\n",
      "Epoch: 060, Train Acc: 0.9267, Test Acc: 0.8947\n",
      "Epoch: 061, Train Acc: 0.9133, Test Acc: 0.8158\n",
      "Epoch: 062, Train Acc: 0.8867, Test Acc: 0.8684\n",
      "Epoch: 063, Train Acc: 0.8533, Test Acc: 0.7632\n",
      "Epoch: 064, Train Acc: 0.9333, Test Acc: 0.8947\n",
      "Epoch: 065, Train Acc: 0.9333, Test Acc: 0.8684\n",
      "Epoch: 066, Train Acc: 0.9000, Test Acc: 0.7895\n",
      "Epoch: 067, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 068, Train Acc: 0.9400, Test Acc: 0.8947\n",
      "Epoch: 069, Train Acc: 0.9067, Test Acc: 0.7895\n",
      "Epoch: 070, Train Acc: 0.9267, Test Acc: 0.8684\n",
      "Epoch: 071, Train Acc: 0.9200, Test Acc: 0.8684\n",
      "Epoch: 072, Train Acc: 0.9267, Test Acc: 0.8684\n",
      "Epoch: 073, Train Acc: 0.9133, Test Acc: 0.7895\n",
      "Epoch: 074, Train Acc: 0.9267, Test Acc: 0.8947\n",
      "Epoch: 075, Train Acc: 0.9333, Test Acc: 0.8947\n",
      "Epoch: 076, Train Acc: 0.9333, Test Acc: 0.8684\n",
      "Epoch: 077, Train Acc: 0.9333, Test Acc: 0.8684\n",
      "Epoch: 078, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 079, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 080, Train Acc: 0.9267, Test Acc: 0.8684\n",
      "Epoch: 081, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 082, Train Acc: 0.9333, Test Acc: 0.8684\n",
      "Epoch: 083, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 084, Train Acc: 0.9267, Test Acc: 0.8684\n",
      "Epoch: 085, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 086, Train Acc: 0.9333, Test Acc: 0.8684\n",
      "Epoch: 087, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 088, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 089, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 090, Train Acc: 0.9267, Test Acc: 0.8684\n",
      "Epoch: 091, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 092, Train Acc: 0.9467, Test Acc: 0.7895\n",
      "Epoch: 093, Train Acc: 0.9333, Test Acc: 0.8684\n",
      "Epoch: 094, Train Acc: 0.9267, Test Acc: 0.8684\n",
      "Epoch: 095, Train Acc: 0.9400, Test Acc: 0.7895\n",
      "Epoch: 096, Train Acc: 0.9333, Test Acc: 0.8684\n",
      "Epoch: 097, Train Acc: 0.9467, Test Acc: 0.8947\n",
      "Epoch: 098, Train Acc: 0.9333, Test Acc: 0.8158\n",
      "Epoch: 099, Train Acc: 0.9467, Test Acc: 0.8421\n",
      "Epoch: 100, Train Acc: 0.9400, Test Acc: 0.8947\n",
      "Epoch: 101, Train Acc: 0.9200, Test Acc: 0.7895\n",
      "Epoch: 102, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 103, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 104, Train Acc: 0.9533, Test Acc: 0.8421\n",
      "Epoch: 105, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 106, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 107, Train Acc: 0.9400, Test Acc: 0.8421\n",
      "Epoch: 108, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 109, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 110, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 111, Train Acc: 0.9467, Test Acc: 0.8421\n",
      "Epoch: 112, Train Acc: 0.9400, Test Acc: 0.8421\n",
      "Epoch: 113, Train Acc: 0.9600, Test Acc: 0.8684\n",
      "Epoch: 114, Train Acc: 0.9400, Test Acc: 0.8421\n",
      "Epoch: 115, Train Acc: 0.9533, Test Acc: 0.8684\n",
      "Epoch: 116, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 117, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 118, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 119, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 120, Train Acc: 0.9600, Test Acc: 0.8684\n",
      "Epoch: 121, Train Acc: 0.9400, Test Acc: 0.8158\n",
      "Epoch: 122, Train Acc: 0.9600, Test Acc: 0.8684\n",
      "Epoch: 123, Train Acc: 0.9533, Test Acc: 0.7632\n",
      "Epoch: 124, Train Acc: 0.9533, Test Acc: 0.7632\n",
      "Epoch: 125, Train Acc: 0.9333, Test Acc: 0.8684\n",
      "Epoch: 126, Train Acc: 0.9533, Test Acc: 0.7895\n",
      "Epoch: 127, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 128, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 129, Train Acc: 0.9600, Test Acc: 0.8421\n",
      "Epoch: 130, Train Acc: 0.9600, Test Acc: 0.8158\n",
      "Epoch: 131, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 132, Train Acc: 0.9533, Test Acc: 0.8684\n",
      "Epoch: 133, Train Acc: 0.9600, Test Acc: 0.8421\n",
      "Epoch: 134, Train Acc: 0.9600, Test Acc: 0.8158\n",
      "Epoch: 135, Train Acc: 0.9600, Test Acc: 0.8684\n",
      "Epoch: 136, Train Acc: 0.9533, Test Acc: 0.8684\n",
      "Epoch: 137, Train Acc: 0.9533, Test Acc: 0.8684\n",
      "Epoch: 138, Train Acc: 0.9333, Test Acc: 0.8684\n",
      "Epoch: 139, Train Acc: 0.9533, Test Acc: 0.7632\n",
      "Epoch: 140, Train Acc: 0.9733, Test Acc: 0.8684\n",
      "Epoch: 141, Train Acc: 0.9533, Test Acc: 0.8421\n",
      "Epoch: 142, Train Acc: 0.9533, Test Acc: 0.7632\n",
      "Epoch: 143, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 144, Train Acc: 0.9467, Test Acc: 0.8421\n",
      "Epoch: 145, Train Acc: 0.9667, Test Acc: 0.8421\n",
      "Epoch: 146, Train Acc: 0.9667, Test Acc: 0.8421\n",
      "Epoch: 147, Train Acc: 0.9667, Test Acc: 0.8684\n",
      "Epoch: 148, Train Acc: 0.9600, Test Acc: 0.8684\n",
      "Epoch: 149, Train Acc: 0.9600, Test Acc: 0.7895\n",
      "Epoch: 150, Train Acc: 0.9600, Test Acc: 0.8684\n",
      "Epoch: 151, Train Acc: 0.9600, Test Acc: 0.8421\n",
      "Epoch: 152, Train Acc: 0.9600, Test Acc: 0.8684\n",
      "Epoch: 153, Train Acc: 0.9000, Test Acc: 0.6842\n",
      "Epoch: 154, Train Acc: 0.8667, Test Acc: 0.6842\n",
      "Epoch: 155, Train Acc: 0.8333, Test Acc: 0.8684\n",
      "Epoch: 156, Train Acc: 0.9400, Test Acc: 0.8421\n",
      "Epoch: 157, Train Acc: 0.8933, Test Acc: 0.6842\n",
      "Epoch: 158, Train Acc: 0.9333, Test Acc: 0.8684\n",
      "Epoch: 159, Train Acc: 0.9133, Test Acc: 0.7632\n",
      "Epoch: 160, Train Acc: 0.9267, Test Acc: 0.8684\n",
      "Epoch: 161, Train Acc: 0.9333, Test Acc: 0.8158\n",
      "Epoch: 162, Train Acc: 0.9467, Test Acc: 0.7632\n",
      "Epoch: 163, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 164, Train Acc: 0.9467, Test Acc: 0.8421\n",
      "Epoch: 165, Train Acc: 0.9400, Test Acc: 0.7895\n",
      "Epoch: 166, Train Acc: 0.9600, Test Acc: 0.8684\n",
      "Epoch: 167, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 168, Train Acc: 0.9333, Test Acc: 0.8684\n",
      "Epoch: 169, Train Acc: 0.9533, Test Acc: 0.8684\n",
      "Epoch: 170, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 171, Train Acc: 0.9600, Test Acc: 0.8684\n",
      "Epoch: 172, Train Acc: 0.9533, Test Acc: 0.8421\n",
      "Epoch: 173, Train Acc: 0.9533, Test Acc: 0.8421\n",
      "Epoch: 174, Train Acc: 0.9533, Test Acc: 0.8421\n",
      "Epoch: 175, Train Acc: 0.9533, Test Acc: 0.8421\n",
      "Epoch: 176, Train Acc: 0.9533, Test Acc: 0.8421\n",
      "Epoch: 177, Train Acc: 0.9467, Test Acc: 0.8421\n",
      "Epoch: 178, Train Acc: 0.9667, Test Acc: 0.8421\n",
      "Epoch: 179, Train Acc: 0.9533, Test Acc: 0.8158\n",
      "Epoch: 180, Train Acc: 0.9533, Test Acc: 0.8158\n",
      "Epoch: 181, Train Acc: 0.9600, Test Acc: 0.8421\n",
      "Epoch: 182, Train Acc: 0.9667, Test Acc: 0.8684\n",
      "Epoch: 183, Train Acc: 0.9533, Test Acc: 0.7895\n",
      "Epoch: 184, Train Acc: 0.9467, Test Acc: 0.8684\n",
      "Epoch: 185, Train Acc: 0.9533, Test Acc: 0.8421\n",
      "Epoch: 186, Train Acc: 0.9733, Test Acc: 0.8158\n",
      "Epoch: 187, Train Acc: 0.9733, Test Acc: 0.7895\n",
      "Epoch: 188, Train Acc: 0.9600, Test Acc: 0.8684\n",
      "Epoch: 189, Train Acc: 0.9400, Test Acc: 0.7895\n",
      "Epoch: 190, Train Acc: 0.9467, Test Acc: 0.8421\n",
      "Epoch: 191, Train Acc: 0.9600, Test Acc: 0.8421\n",
      "Epoch: 192, Train Acc: 0.9533, Test Acc: 0.8421\n",
      "Epoch: 193, Train Acc: 0.9733, Test Acc: 0.8421\n",
      "Epoch: 194, Train Acc: 0.9667, Test Acc: 0.8684\n",
      "Epoch: 195, Train Acc: 0.9533, Test Acc: 0.7105\n",
      "Epoch: 196, Train Acc: 0.9600, Test Acc: 0.8684\n",
      "Epoch: 197, Train Acc: 0.9667, Test Acc: 0.8684\n",
      "Epoch: 198, Train Acc: 0.9667, Test Acc: 0.7895\n",
      "Epoch: 199, Train Acc: 0.9667, Test Acc: 0.8421\n",
      "Epoch: 200, Train Acc: 0.9600, Test Acc: 0.7895\n"
     ]
    }
   ],
   "source": [
    "model = GNN(hidden_channels=64)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-backdoor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
